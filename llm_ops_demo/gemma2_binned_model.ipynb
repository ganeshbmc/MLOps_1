{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73ae4bb-f2bf-4a1a-8971-3c107bdd45ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install libraries \n",
    "\n",
    "First, install the libraries to be able to load fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22085bc4-c678-46db-958a-0e9636537a42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf44e82f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cu124'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b13472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9422faa7-e95a-4bb7-9ac8-efa3ece3e662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f623d099-c467-487f-ab10-5c6a286a5dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gemma-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.55.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c42e4c-222a-4a57-a043-870ce40df75a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gemma-env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2ced818-59cb-4751-822b-83aea2c4e1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c59aaa-7cad-4c98-bd4c-cbf26afbd014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g2_binned_model = \"gs://mlops-course-polar-pillar-461115-g2-week10-v2/fine-tuning/output/gemma2-2b-it-1755414947671-20250817010839/merged_model\"\n",
    "local_dir = \"g2_binned_model\"\n",
    "model_name = \"g2_binned_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb82adb-e0b3-401b-829e-5da0846ff404",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/g2_binned_model\n"
     ]
    }
   ],
   "source": [
    "!echo models/$local_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf1d59-cc5b-4663-9750-777d444bf81c",
   "metadata": {},
   "source": [
    "### Run the following cell only if you want to reimport the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9993f166-97e0-46b4-95d0-e95e6b9c413d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf models/$local_dir\n",
    "# !mkdir -p models/$local_dir\n",
    "# !gsutil -m cp -r $g2_binned_model models/$local_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fdcad3",
   "metadata": {},
   "source": [
    "### Vertex AI Workbench    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01226b85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/iris_pipeline/llm_ops_demo/models/g2_binned_model'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path for local VS code set up\n",
    "import os\n",
    "# local_dir = os.path.join(os.getcwd(), r\"models\\g3_plain_model\")\n",
    "local_dir = os.path.join(os.getcwd(), f\"models/{model_name}\")\n",
    "local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d96427de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/iris_pipeline/llm_ops_demo/models/g2_binned_model/merged_model'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Check local_dir path\n",
    "import os\n",
    "os.path.join(local_dir, \"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "139d94f5-f34d-482a-8649-984f6f4eeae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 17.76s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    (rotary_emb): Gemma2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "local_path = os.path.join(local_dir, \"merged_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e60b0e55-7713-4e08-9c40-a598bb94c8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gemma3_prompt(text):\n",
    "    prompt = (\n",
    "    \"<start_of_turn>system\\n\"\n",
    "    \"Classify the flower based on its measurements into one of the following species: [setosa, versicolor, virginica]\\n\"\n",
    "    \"<end_of_turn>\\n\"\n",
    "    \"<start_of_turn>user\\n\"\n",
    "    +text+\n",
    "    \"<end_of_turn>\\n\"\n",
    "    \"<start_of_turn>assistant\\n\"\n",
    "    )\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77951067-0118-49f4-a720-de167e084b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Text for g3_plain_model\n",
    "# text= (\n",
    "#     # \"Sepal Length: 6.4, Sepal Width: 2.9, Petal Length: 4.3, Petal Width: 1.3\" #versicolor\n",
    "#     # \"Sepal Length: 5.0, Sepal Width: 3.6, Petal Length: 1.4, Petal Width: 0.2\" #setosa\n",
    "#     \"Sepal Length: 5.9, Sepal Width: 3.0, Petal Length: 5.1, Petal Width: 1.8\" #virginica\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "859a87dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text for g3_binned_model\n",
    "text= (\n",
    "    # \"Sepal Length: medium, Sepal Width: small, Petal Length: medium, Petal Width: medium\" #versicolor\n",
    "    # \"Sepal Length: small, Sepal Width: medium, Petal Length: small, Petal Width: small\" #setosa\n",
    "    \"Sepal Length: large, Sepal Width: medium, Petal Length: large, Petal Width: large\" #virginica\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbb924f6-9ad1-4fa6-a446-e2dcf841f206",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/iris_pipeline/llm_ops_demo/models/g2_binned_model'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99c8cea2-18e6-454c-a1b3-cb08e7b577eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: virginica\n",
      "Model used: g2_binned_model\n",
      "\n",
      "Prompt used:\n",
      "<start_of_turn>system\n",
      "Classify the flower based on its measurements into one of the following species: [setosa, versicolor, virginica]\n",
      "<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Sepal Length: large, Sepal Width: medium, Petal Length: large, Petal Width: large<end_of_turn>\n",
      "<start_of_turn>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = gemma3_prompt(text)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# Decode only the new tokens\n",
    "generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "print(\"Predicted class:\", generated_text.strip())\n",
    "print(f\"Model used: {local_dir.split('/')[-1]}\")\n",
    "print(f\"\\nPrompt used:\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7f7b15d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 labels: ['setosa', 'setosa', 'virginica', 'virginica', 'virginica', 'setosa', 'versicolor', 'setosa', 'virginica', 'setosa']\n",
      "Total labels: 60\n"
     ]
    }
   ],
   "source": [
    "# Load iris_binned_v1_test.jsonl to prepare y_test\n",
    "import json\n",
    "\n",
    "file_path = \"data/iris_binned_v1_test.jsonl\"\n",
    "\n",
    "labels = []\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        for msg in record[\"messages\"]:\n",
    "            if msg[\"role\"] == \"assistant\":\n",
    "                labels.append(msg[\"content\"].strip())\n",
    "\n",
    "print(\"First 10 labels:\", labels[:10])\n",
    "print(\"Total labels:\", len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "262586bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def predict_jsonl(\n",
    "    jsonl_path: str,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    gemma3_prompt,\n",
    "    max_new_tokens: int = 2,\n",
    "    do_sample: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Iterates over a JSONL with records like:\n",
    "      {\"messages\":[{\"role\":\"system\",\"content\":\"...\"}, {\"role\":\"user\",\"content\":\"...\"}, {\"role\":\"assistant\",\"content\":\"label\"}]}\n",
    "    Extracts the user content (fallback to system), builds prompt, generates prediction,\n",
    "    and appends to `predictions`.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Prefer user content; fallback to system if user is missing\n",
    "            user_text = next((m[\"content\"] for m in obj.get(\"messages\", []) if m.get(\"role\") == \"user\"), None)\n",
    "            if user_text is None:\n",
    "                user_text = next((m[\"content\"] for m in obj.get(\"messages\", []) if m.get(\"role\") == \"system\"), \"\")\n",
    "\n",
    "            prompt = gemma3_prompt(user_text.strip())\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=do_sample,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            # Decode only the newly generated tokens\n",
    "            new_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "            pred = tokenizer.decode(new_tokens, skip_special_tokens=True).strip().lower()\n",
    "            predictions.append(pred)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6e3cabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'setosa', 'virginica', 'virginica', 'virginica', 'setosa', 'versicolor', 'setosa', 'virginica', 'setosa']\n",
      "Total predictions: 60\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_jsonl(\n",
    "    \"data/iris_binned_v1_test.jsonl\",\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    gemma3_prompt=gemma3_prompt,\n",
    "    max_new_tokens=2,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "print(predictions[:10])\n",
    "print(\"Total predictions:\", len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47c075b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0]\n",
      " [ 0 17  3]\n",
      " [ 0  1 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        20\n",
      "  versicolor       0.94      0.85      0.89        20\n",
      "   virginica       0.86      0.95      0.90        20\n",
      "\n",
      "    accuracy                           0.93        60\n",
      "   macro avg       0.94      0.93      0.93        60\n",
      "weighted avg       0.94      0.93      0.93        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix, classification report \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(confusion_matrix(labels, predictions))\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f434f8-5d74-4c72-99f4-c77c215d5ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "gema-env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (Gemma-GPU)",
   "language": "python",
   "name": "gema-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
